{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generator function \n",
    "\n",
    "class sample_generator():\n",
    "    def __init__(self,dataframe):\n",
    "        \n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "    def next_batch():\n",
    "        \"\"\" augment and return batch\"\"\"\n",
    "        \n",
    "        X_batch, ybatch =  batch_from_prefetch_samples(dataframe,self.batchsize)\n",
    "        return augment_batch(X_batch) , y_batch\n",
    "        \n",
    "        \n",
    "    def batch_from_prefetch_samples():\n",
    "        dataframe = self.dataframe\n",
    "        batchsize = self.batchsize\n",
    "        y_batch = []\n",
    "        X_batch = []\n",
    "    \n",
    "        serie = dataframe.sample(n = batchsize, replace = True )\n",
    "\n",
    "        y_batch = serie[0].tolist()\n",
    "        X_batch = serie[1].tolist()\n",
    "\n",
    "\n",
    "        return  X_batch, y_batch\n",
    "\n",
    "        \n",
    "        \n",
    "    def augment_batch(X_batch, add_shift = True, add_noise = True):\n",
    "            \"\"\" make some noise, shift the data around \"\"\"\n",
    "            \n",
    "            shift_max = self.shift_max\n",
    "            noise_max = self.noise_max\n",
    "            \n",
    "            target_length = X_batch[0].shape[0]\n",
    "    \n",
    "            X_batch_out = []\n",
    "    \n",
    "            for X_elem in X_batch:\n",
    "        \n",
    "                if add_shift:\n",
    "                    shift = random.randint(0,shift_max)\n",
    "            #print(shift)\n",
    "                else:\n",
    "                    shift = 0\n",
    "                deletion = []\n",
    "                end = 0\n",
    "                start = 1\n",
    "        \n",
    "            # cut randomly in the front and back of the columns\n",
    "                for f in range(shift):\n",
    "                    if random.uniform(0.1, 1.1) > 0.55:\n",
    "                        deletion.append(  X_elem.shape[0] - (end+1) )\n",
    "                \n",
    "                        #X_elem = np.delete(X_elem,(X_elem.shape[0] - (end+1),0)\n",
    "\n",
    "                        end +=1\n",
    "                    else:\n",
    "                        deletion.append(start)\n",
    "                        #X_elem = np.delete(X_elem,(start),0)\n",
    "        \n",
    "                        start +=1\n",
    "        \n",
    "                X_elem = np.delete(X_elem,deletion,0)\n",
    "                # randomly add the missing columns to the end or beginning of the dataset\n",
    "        \n",
    "                missing = target_length - X_elem.shape[0]\n",
    "                if missing > 0 :\n",
    "                    if random.uniform(0, 1) > 0.5:\n",
    "                        # add to the front\n",
    "                        X_elem = np.vstack((X_elem,np.random.rand(missing,13)))\n",
    "                    \n",
    "                    else:\n",
    "                        # add to the back\n",
    "                        X_elem = np.vstack((np.random.rand(missing,13), X_elem))\n",
    "                    \n",
    "                    if add_noise:\n",
    "                        noise = np.random.normal(0,noise_max,(99*13)).reshape((99, 13))\n",
    "                    #X_batch_out.append(np.add(noise,X_elem))\n",
    "            \n",
    "                else:\n",
    "                    missing = X_elem.shape[0] -target_length \n",
    "                    \n",
    "                    \n",
    "                delete= []\n",
    "                for element in range(missing):\n",
    "                    delete.append(X_elem.shape[0] - (element) )\n",
    "                    #print(\"element too long,removing one \")\n",
    "                    X_elem = np.delete(X_elem,delete,0)\n",
    "\n",
    "                if add_noise:\n",
    "                    noise = np.random.normal(0,noise_max,(99*13)).reshape((99, 13))\n",
    "                    \n",
    "            try:\n",
    "                X_batch_out.append(np.add(noise,X_elem))\n",
    "            except:\n",
    "                X_elem = np.delete(X_elem,X_elem.shape[0]-1,0)\n",
    "                X_batch_out.append(np.add(noise,X_elem))\n",
    "    \n",
    "            return X_batch_out\n",
    "        #\n",
    "        #X_out = np.add(X_elem,noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write that thing into a class, for optimized optimization\n",
    "\n",
    "\n",
    "class RNN_BN():\n",
    "    def __init__(self, exampledata , learning_rate=0.0002, base_neurons = 40, momentum = 0.99, dropout = 0.4, batchsize = 100, outputs = 11\n",
    "                rnn_layers = 3 , dense_layers = 2, n_epochs = 100, \n",
    "                 divisionfactor = 4 ,autorun = True, timecourse = False, \n",
    "                 savename = 'default_RNN', savethreshold = 0.9, n_filters = 20):\n",
    "        \n",
    "        # feed an example data, a list of ndarrays, generate tensors based on dimensions of first element\n",
    "        # placeholders\n",
    "        self.X = tf.placeholder(tf.float32,[None,exampledata[0][0],exampledata[0][1]])\n",
    "        self.y = tf.placeholder(tf.int32,[None])\n",
    "        self.learning_rate = learning_rate\n",
    "        self.base_neurons = base_neurons\n",
    "        self.momentum = momentum\n",
    "        self.dropout = dropout\n",
    "        self.batchsize = batchsize\n",
    "        self.outputs = outputs\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.dense_layers = dense_layers\n",
    "        self.timecourse = timecourse\n",
    "        self.savename = savename\n",
    "        self.n_epochs = n_epochs\n",
    "        self.divisionfactor = dicisionfactor\n",
    "        self.savethreshold = savethreshold\n",
    "        self.n_filters = n_filters\n",
    "        \n",
    "        # autorun constructs \n",
    "        \n",
    "        if autorun:\n",
    "            construct()\n",
    "        \n",
    "        \n",
    "    def construct(self):\n",
    "        # clean the default graph\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        \n",
    "        \n",
    "        # training tensor\n",
    "        training = tf.placeholder_with_default(False,shape=(),name=\"training\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        cells=[tf.contrib.rnn.GRUCell(num_units=n_neurons,activation=tf.nn.relu)]\n",
    "        for c in range(self.rnn_layers):\n",
    "            cells.append(tf.contrib.rnn.DropoutWrapper( tf.contrib.rnn.GRUCell(num_units=(n_neurons),\n",
    "                                                                               activation=tf.nn.relu, \n",
    "                                                                               kernel_initializer = tf.contrib.layers.xavier_initializer()) \n",
    "                                                       ,input_keep_prob = (1-dropout) )  )\n",
    "        \n",
    "        #within scope assemble the multilayer cell\n",
    "        with tf.name_scope(\"rnn\"):\n",
    "            multi_layer_cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=False)\n",
    "            \n",
    "            # dynamic RNN creates graphs faster than the static rnn \n",
    "            outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, self.X, dtype=tf.float32)\n",
    "            \n",
    "\n",
    "        with tf.name_scope(\"supplement_layer\"):\n",
    "            # dense layer  try states (last information of RNN  or outputs,  all information)\n",
    "            \n",
    "            if(timecourse):\n",
    "                outputs_flat = tf.reshape(outputs, [-1, (n_neurons * n_inputs)])\n",
    "            else:\n",
    "                outputs_flat = states\n",
    "            \n",
    "            dense = tf.layers.dense(outputs_flat, (n_neurons - 5),\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    name=\"supplementationDense1\") # add a dense layer\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "               # batch normalization\n",
    "            bn_norm_sup = tf.layers.batch_normalization(dense,training=training, momentum = momentum)\n",
    "            bn_norm_act = tf.nn.elu(bn_norm_sup) # activate the batch norm\n",
    "    \n",
    "            drop1 = tf.layers.dropout(bn_norm_act, training = training, rate=0.5 )\n",
    "\n",
    "            dense2 = tf.layers.dense(drop1, (n_neurons - 15), activation=tf.nn.elu,\n",
    "                                     kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     name=\"supplementationDense2\") # add a dense layer\n",
    "    \n",
    "               # batch normalization\n",
    "            bn_norm_sup2 = tf.layers.batch_normalization(dense2,training=training, momentum = momentum)\n",
    "                             \n",
    "            bn_norm_act2 = tf.nn.elu(bn_norm_sup2) # activate the batch norm\n",
    "    \n",
    "            #drop2 = tf.layers.dropout(bn_norm_act2, training = training, rate=0.5 )\n",
    "\n",
    "            logits_before_bn = tf.layers.dense(bn_norm_act2, n_outputs, name=\"outputs_logits\") #logits = tf.layers.dense(states, n_outputs)\n",
    "            logits = tf.layers.batch_normalization(logits_before_bn,training=training, momentum = momentum)\n",
    "    \n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # sparse softmax, so I don't have to get the ohot\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            loss = tf.reduce_mean(xentropy)\n",
    "    \n",
    "    \n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "        with tf.name_scope(\"predict\"):\n",
    "            prediction = tf.argmax(logits,1)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "            \n",
    "        def run_model(self,fulldatasize = 1000000,generator,X_test,y_test):\n",
    "            \n",
    "            with tf.Session() as sess:\n",
    "                init.run()\n",
    "                \n",
    "                for epoch in range(n_epochs):\n",
    "                    for iteration in range( fulldatasize // (batch_size*40) ):\n",
    "                        X_batch, y_batch = generator.next()\n",
    "                        X_batch = augment_batch(X_batch, shift_max=12, noise_max=3.5)\n",
    "                        \n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                    acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    acc_mess = accuracy.eval(feed_dict={X: X_mess, y: y_mess})\n",
    "                    if acc_mess > self.savethreshold :\n",
    "                        save_path = saver.save(sess, \"%_checkpoint%s.ckpt\" %(self.savename, acc_mess) )\n",
    "            \n",
    "                    if verbose:\n",
    "                        print(epoch, \"Train accuracy:\", acc_train, \" Test accuracy \", acc_mess)  \n",
    "                        \n",
    "                acc_mess = accuracy.eval(feed_dict={X: X_mess, y: y_mess})\n",
    "                save_path = saver.save(sess, \"%_final_%s.ckpt\" %(self.savename, acc_mess) )\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "     \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
