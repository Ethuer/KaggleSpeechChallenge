{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Multilayer_RNN():\n",
    "    def __init__(self, an_inputs = 99*13,an_hidden1 = 800,\n",
    "                 an_hidden2 = 550, an_hiddencore = 200,a_learning_rate_initial = 0.00007,\n",
    "                 a_decay_steps = 10000,a_decay_rate = 0.95,\n",
    "                 l2_reg = 0.0004,conv_learning_rate_init = 0.002,conv_decay_steps = 10000,\n",
    "                 conv_decay_rate = 0.95,conv_learning_rate = 0.002,dropout = 0.5,\n",
    "                 input_keep_prob = 0.3,n_steps = 13,n_inputs = 99,\n",
    "                 n_neurons = 300 ,n_outputs = 7,\n",
    "                 initial_learning = 0.00015,decay_steps = 30000,\n",
    "                 decay_rate = 0.98,momentum = 0.995 ,scale =0.0014,\n",
    "                logdir = 'Tensorboard_logdir'):\n",
    "        \n",
    "        \n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        \n",
    "        self.logdir = logdir\n",
    "        \n",
    "        # autoencoder layers\n",
    "        self.an_inputs = an_inputs\n",
    "        self.an_hidden1 = an_hidden1\n",
    "        self.an_hidden2 = an_hiddencore\n",
    "        self.an_hiddencore = an_hiddencore\n",
    "        self.an_hiddencoreout = an_hiddencore\n",
    "        self.an_hidden3 = an_hidden1\n",
    "        self.an_outputs = an_inputs\n",
    "\n",
    "        # autoencoder parameters\n",
    "        \n",
    "        self.a_learning_rate_initial = a_learning_rate_initial = 0.00007 \n",
    "        self.a_decay_steps = a_decay_steps\n",
    "        self.a_decay_rate = a_decay_rate\n",
    "\n",
    "        self.l2_reg = l2_reg\n",
    "        self.activation = tf.nn.elu\n",
    "        self.regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "        self.initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "        \n",
    "        # Convolutional parameters\n",
    "        self.conv_learning_rate_init = conv_learning_rate_init\n",
    "        self.conv_decay_steps = conv_decay_rate\n",
    "        self.conv_decay_rate = conv_decay_rate\n",
    "        self.conv_learning_rate = conv_learning_rate\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # RNN parameters\n",
    "        self.n_steps = n_steps\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        self.n_outputs = n_outputs\n",
    "        self.initial_learning = initial_learning\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_rate = decay_rate\n",
    "        self.momentum = momentum\n",
    "        self.scale = scale\n",
    "        self.input_keep_prob = input_keep_prob\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[None,99,13] )\n",
    "        self.y = tf.placeholder(tf.int32,[None])\n",
    "        \n",
    "        self.training = tf.placeholder_with_default(False,shape=(),name='training')\n",
    "        self.a_training = tf.placeholder_with_default(False,shape=(),name='AENCtraining')\n",
    "        self.train_convNet = tf.placeholder_with_default(False,shape=(),name='CONVtraining')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def _build(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Autoencoder first,  tying input output weights, since tf layers don't really support this, manipulate the weights directly\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.a_global_step = tf.Variable(0,trainable=False)\n",
    "\n",
    "        \n",
    "        with tf.name_scope(\"Autoencoder\"):\n",
    "            #  Autoencoder generation.  Can just run in same graph, mostly because importing is annoying\n",
    "\n",
    "            self.a_weights1_init = self.initializer([self.an_inputs, self.an_hidden1])\n",
    "            self.a_weights2_init = self.initializer([self.an_hidden1, self.an_hidden2])\n",
    "            self.a_weightscore_init = self.initializer([self.an_hidden2, self.an_hiddencore])\n",
    "\n",
    "            # weights for autoencoder,  linking 1&4 and 2&3  so I only need to train 1 and 2\n",
    "            self.a_weights1 = tf.Variable(self.a_weights1_init, dtype=tf.float32, name=\"a_weights1\")\n",
    "            self.a_weights2 = tf.Variable(self.a_weights2_init, dtype=tf.float32, name=\"a_weights2\")\n",
    "            self.a_weightscore = tf.Variable(self.a_weightscore_init, dtype=tf.float32, name=\"a_weightscore\")\n",
    "\n",
    "            self.a_weightscoreout = tf.transpose(self.a_weightscore, name=\"a_weightscoreout\") # tied weights\n",
    "            self.a_weights3 = tf.transpose(self.a_weights2, name=\"a_weights3\")  # tied weights\n",
    "            self.a_weights4 = tf.transpose(self.a_weights1, name=\"a_weights4\")  # tied weights\n",
    "\n",
    "            # biases for autoencoder,  not linked\n",
    "            self.a_biases1 = tf.Variable(tf.zeros(self.an_hidden1), name=\"a_biases1\")\n",
    "            self.a_biases2 = tf.Variable(tf.zeros(self.an_hidden2), name=\"a_biases2\")\n",
    "            self.a_biasescore = tf.Variable(tf.zeros(self.an_hiddencore), name=\"a_biasescore\")\n",
    "            self.a_biasescoreout = tf.Variable(tf.zeros(self.an_hiddencoreout), name=\"a_biasescoreout\")\n",
    "            self.a_biases3 = tf.Variable(tf.zeros(self.an_hidden3), name=\"a_biases3\")\n",
    "            self.a_biases4 = tf.Variable(tf.zeros(self.an_outputs), name=\"a_biases4\")\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"pretraining_Autoencoder\"):\n",
    "            \n",
    "            \n",
    "            self.X_flat = tf.contrib.layers.flatten(self.X)\n",
    "            \n",
    "            \n",
    "            self.a_X_drop = tf.layers.dropout(inputs=self.X_flat,rate=self.dropout,training=self.a_training)\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.a_hidden1 = self.activation(tf.matmul(self.a_X_drop, self.a_weights1) + self.a_biases1)\n",
    "            self.a_hidden2 = self.activation(tf.matmul(self.a_hidden1, self.a_weights2) + self.a_biases2)\n",
    "    \n",
    "            self.a_hiddencore = self.activation(tf.matmul(self.a_hidden2, self.a_weightscore) + self.a_biasescore)\n",
    "    \n",
    "            #a_hiddencoreout = activation(tf.matmul(a_stop, a_weightscoreout) + a_biasescoreout)\n",
    "            self.a_hiddencoreout = self.activation(tf.matmul(self.a_hiddencore, self.a_weightscoreout) + self.a_biasescoreout)\n",
    "    \n",
    "            self.a_hidden3 = self.activation(tf.matmul(self.a_hiddencoreout, self.a_weights3) + self.a_biases3)\n",
    "            self.a_outputs = tf.matmul(self.a_hidden3, self.a_weights4) + self.a_biases4\n",
    "\n",
    "            self.a_outputs_reshaped = tf.reshape(self.a_outputs, shape=[-1,99,13])\n",
    "    \n",
    "            self.a_reconstruction_loss = tf.reduce_mean(tf.square(self.a_outputs - self.X_flat))\n",
    "            self.a_reg_loss = self.regularizer(self.a_weights1) + self.regularizer(self.a_weights2)\n",
    "            self.a_loss = self.a_reconstruction_loss + self.a_reg_loss\n",
    "\n",
    "            self.a_learning_rate = tf.train.natural_exp_decay(self.a_learning_rate_initial,self.a_global_step,self.a_decay_steps,self.a_decay_rate)\n",
    "            self.a_optimizer = tf.train.AdamOptimizer(self.a_learning_rate)\n",
    "            self.a_training_op = self.a_optimizer.minimize(self.a_loss, global_step=self.a_global_step)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.cells = [\n",
    "        \n",
    "        tf.contrib.rnn.GRUCell(num_units=self.n_neurons,activation=tf.nn.tanh), #relu outperforms elu but tanh matches the activation range\n",
    "        \n",
    "         \n",
    "         tf.contrib.rnn.DropoutWrapper(\n",
    "         tf.contrib.rnn.GRUCell(num_units=(self.n_neurons-10),activation=tf.nn.tanh,  # relus get > 88%\n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "          , input_keep_prob = self.input_keep_prob ),\n",
    "    \n",
    "        tf.contrib.rnn.DropoutWrapper(\n",
    "         tf.contrib.rnn.GRUCell(num_units=(self.n_neurons-30),activation= tf.nn.relu, #tf.nn.tanh,  # relus get > 88%\n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "          , input_keep_prob =self.input_keep_prob),\n",
    "    \n",
    "\n",
    "        tf.contrib.rnn.OutputProjectionWrapper(\n",
    "            \n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "         tf.contrib.rnn.GRUCell(num_units=(self.n_neurons-10),activation=tf.nn.relu, #tf.nn.tanh,  # relus get > 88%\n",
    "                               kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "          , input_keep_prob = self.input_keep_prob),\n",
    "         \n",
    "            output_size = self.n_outputs\n",
    "        )\n",
    "\n",
    "    \n",
    "        ]\n",
    "        \n",
    "        with tf.name_scope(\"rnn\"):\n",
    "    \n",
    "            # 2 layer GRUs work to 90% singleaccurary with some Augmentation on all training data,\n",
    "            # needs a few more layers to generalize the unwanted labels to unknown\n",
    "            self.multi_layer_cell = tf.contrib.rnn.MultiRNNCell(self.cells, state_is_tuple=False)\n",
    "    \n",
    "            # both outputs ( states over times) and states ( final neuron states ) contain information, so using both of those. \n",
    "            self.outputs, self.states = tf.nn.dynamic_rnn(self.multi_layer_cell, self.X, dtype=tf.float32)\n",
    "    \n",
    "            self.outputs_flat = tf.reshape(self.outputs, [-1, ( self.n_inputs*12)])\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"Conv\"):\n",
    "    \n",
    "\n",
    "            self.conv_global_step = tf.Variable(0, trainable=False)\n",
    "            self.a_output_stop_fromconv = tf.stop_gradient(self.a_outputs_reshaped)\n",
    "            self.input_2d =  tf.expand_dims(self.a_output_stop_fromconv,3)\n",
    "            self.normresponse_input = tf.nn.local_response_normalization(self.input_2d)\n",
    "    \n",
    "            # save memory space by reducing resolution\n",
    "            self.low_res_input = tf.nn.max_pool(self.input_2d,ksize=[1,2, 2,1], strides=[1,2, 2,1], padding=\"SAME\")\n",
    "\n",
    "            self.conv_layer = tf.layers.conv2d(self.low_res_input ,\n",
    "                                          filters=32,\n",
    "                                          kernel_size = [2,2],\n",
    "                                          padding='SAME', \n",
    "                                  activation=tf.nn.tanh )\n",
    " \n",
    "            self.normresponse_conv_1 = tf.nn.local_response_normalization(self.conv_layer)\n",
    "\n",
    "            self.max_pool_in = tf.nn.max_pool(self.normresponse_conv_1, ksize=[1,2, 2,1], strides=[1,2, 2,1], padding=\"SAME\",)\n",
    "    \n",
    "            self.normresponse = tf.nn.local_response_normalization(self.max_pool_in)\n",
    "    \n",
    "            self.conv_layer2 = tf.layers.conv2d(self.normresponse ,filters=32,kernel_size = [3,2],dilation_rate=2, padding='SAME')\n",
    "            self.conv_layer_elu2 = tf.nn.elu(self.conv_layer2)\n",
    "    \n",
    "            self.normresponse_2 = tf.nn.local_response_normalization(self.conv_layer_elu2)\n",
    "    \n",
    "            self.conv_layer3 = tf.layers.conv2d(self.normresponse_2 ,filters=32,kernel_size = [3,2],dilation_rate=2, padding='SAME')\n",
    "            self.conv_layer_elu3 = tf.nn.elu(self.conv_layer3) \n",
    "    \n",
    "            self.normresponse_3 = tf.nn.local_response_normalization(self.conv_layer_elu3)\n",
    "    \n",
    "    \n",
    "            self.pool_flat_in = tf.contrib.layers.flatten(self.normresponse_3)\n",
    "    \n",
    "            self.act_bn_norm = tf.layers.batch_normalization(self.pool_flat_in,training=self.train_convNet, momentum = self.momentum)\n",
    "                             \n",
    "            self.act_bn_norm_act = tf.nn.elu(self.act_bn_norm)\n",
    "    \n",
    "            self.act_elu = tf.layers.dense(self.act_bn_norm_act, units=self.n_neurons, activation=tf.nn.elu)\n",
    "    \n",
    "            self.act_drop = tf.layers.dropout(self.act_elu, training = self.train_convNet, rate=self.dropout )\n",
    "    \n",
    "            self.act_elu_stage2 = tf.layers.dense(self.act_drop, units=self.n_neurons - 15, activation=tf.nn.elu)\n",
    "    \n",
    "            self.act_drop2 = tf.layers.dropout(self.act_elu_stage2, training = self.train_convNet, rate=self.dropout )\n",
    "    \n",
    "    \n",
    "            # outgoing layer, stops the gradient \n",
    "            self.conv_stop = tf.stop_gradient(self.act_drop2) # don't propagate the gradient back any further\n",
    "            self.conv_outgoing = tf.layers.dense(self.conv_stop, units= self.n_neurons - 15 )\n",
    "    \n",
    "            # training and evaluation for the Convnet alone\n",
    "    \n",
    "            self.conv_logits = tf.layers.dense(self.act_drop2, self.n_outputs, name=\"conv_outputs_logits\") \n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"pretraining_Convolutional\"):\n",
    "            self.conv_xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=self.conv_logits)\n",
    "            self.conv_loss = tf.reduce_mean(self.conv_xentropy)\n",
    "            self.conv_optimizer = tf.train.AdamOptimizer(learning_rate=self.conv_learning_rate)# Adamoptimizer implementing the nesterov step \n",
    "            self.conv_training_op = self.conv_optimizer.minimize(self.conv_loss)\n",
    "            self.conv_correct = tf.nn.in_top_k(self.conv_logits, self.y, 1)\n",
    "            self.conv_accuracy = tf.reduce_mean(tf.cast(self.conv_correct, tf.float32))\n",
    "    \n",
    "        \n",
    "        with tf.name_scope(\"DenseFromEncoder\"):\n",
    "            self.encoder_stop = tf.stop_gradient(self.a_hiddencore)\n",
    "    \n",
    "            self.input_encoder = self.encoder_stop # using the central layer of the encoder\n",
    "    \n",
    "            self.first_encoder = tf.layers.dense(self.input_encoder, units=self.n_neurons, activation=tf.nn.elu)\n",
    "\n",
    "        with tf.name_scope(\"parted_layer_output\"):\n",
    "    \n",
    "            # outputs are a tensor of  batch,timepoints(99),cffs(13)\n",
    "            # feeding this into a 3d convnet need to add channels1  tf.expand_dims(t, 1)\n",
    "            self.output_2d = tf.expand_dims(self.outputs,3)\n",
    "            self.low_res_RNN_out = tf.nn.avg_pool(self.output_2d,ksize=[1,2, 2,1], strides=[1,2, 2,1], padding=\"SAME\")\n",
    "            # reduce resolution\n",
    "    \n",
    "    \n",
    "            self.conv2d = tf.layers.conv2d(self.low_res_RNN_out,filters=20,kernel_size = [3,3],strides=[1,1], padding='SAME')\n",
    "            self.convoutput_2d_act = tf.nn.elu(self.conv2d)\n",
    "    \n",
    "            self.normresponse_out1 = tf.nn.local_response_normalization(self.convoutput_2d_act)\n",
    "    \n",
    "            self.conv2d_2 = tf.layers.conv2d(self.normresponse_out1,filters=20,kernel_size = [3,3],strides=[1,1], padding='SAME')\n",
    "            self.convoutput_2d_act_2 = tf.nn.elu(self.conv2d_2)\n",
    "    \n",
    "    \n",
    "            self.normresponse_out1 = tf.nn.local_response_normalization(self.convoutput_2d_act_2)\n",
    "            # 99 x 12   => 33 x6\n",
    "            # 99 x 12  [3,6] => 33*2\n",
    "            self.max_pool = tf.nn.max_pool(self.normresponse_out1, ksize=[1,3, 3,1], strides=[1,3, 3,1], padding=\"SAME\")\n",
    "\n",
    "            self.pool_flat = tf.contrib.layers.flatten(self.max_pool)\n",
    "    \n",
    "            self.dense_out = tf.layers.dense(self.pool_flat, (self.n_neurons), #activation=tf.nn.elu,  \n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                           name=\"supplementationDense_fullRNNoutput\") # add a dense layer\n",
    "                         \n",
    "        with tf.name_scope(\"parted_layer_states\"):   \n",
    "    \n",
    "            self.dense_state = tf.layers.dense(self.states, (self.n_neurons), activation=tf.nn.elu,  \n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                           name=\"supplementationDense_covering_states\") # add a dense layer\n",
    "    \n",
    "    \n",
    "        with tf.name_scope(\"combine_states\"):\n",
    "    \n",
    "    \n",
    "            self.combinedstates = tf.concat([self.dense_state, self.dense_out,self.conv_outgoing], 1)\n",
    "\n",
    "        \n",
    "        with tf.name_scope(\"supplement_layer\"):\n",
    "   \n",
    "   \n",
    "            self.dense2 = tf.layers.dense(self.combinedstates, (self.n_neurons * 2 ), activation=tf.nn.elu,  \n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            \n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(self.scale),\n",
    "                           name=\"supplementationDense2\") # add a dense layer\n",
    "    \n",
    "    #batch normalization  \n",
    "    # doesn't improve output applied before skip connections\n",
    "    \n",
    "    \n",
    "            self.bn_norm_sup2 = tf.layers.batch_normalization(self.dense2,training=self.training, momentum = self.momentum)\n",
    "                             \n",
    "            self.bn_norm_act2 = tf.nn.elu(self.bn_norm_sup2) # activate the batch norm\n",
    "    \n",
    "    \n",
    "     # combine with states here \n",
    "        \n",
    "            self.drop2_ext = tf.concat([self.bn_norm_act2, self.states], 1)\n",
    "    \n",
    "    # addition\n",
    " \n",
    "    \n",
    "            self.bn_norm_sup_ext = tf.layers.batch_normalization(self.drop2_ext,training=self.training, momentum = self.momentum)\n",
    "                             \n",
    "            self.bn_norm_act_ext = tf.nn.elu(self.bn_norm_sup_ext)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            self.dense3 = tf.layers.dense(self.bn_norm_act_ext, (self.n_neurons), activation=tf.nn.elu,  \n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(self.scale),\n",
    "                           name=\"supplementationDense3\") # add a dense layer\n",
    "    \n",
    "    \n",
    "    # add autoencoder data here\n",
    "    \n",
    "            self.dense_encoded = tf.concat([self.dense3, self.first_encoder], 1)\n",
    "    \n",
    "    \n",
    "            self.bn_norm_enc_ext = tf.layers.batch_normalization(self.dense_encoded,training=self.training, momentum = self.momentum)\n",
    "                             \n",
    "            self.bn_norm_enc_ext = tf.nn.elu(self.bn_norm_enc_ext)\n",
    "    \n",
    "            self.dense3_encode = tf.layers.dense(self.bn_norm_enc_ext, (self.n_neurons), activation=tf.nn.elu,  \n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(self.scale),\n",
    "                           name=\"supplementationDense3enc\") # add a dense layer\n",
    "    \n",
    "            self.drop3 = tf.layers.dropout(self.dense3_encode, training = self.training, rate=(1-self.input_keep_prob))\n",
    "    \n",
    "            self.dense3_b = tf.layers.dense(self.drop3, (self.n_neurons), activation=tf.nn.elu,  \n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(self.scale),\n",
    "                           name=\"supplementationDense3_b\") # add a dense layer\n",
    "    \n",
    "            self.bn_norm_sup3 = tf.layers.batch_normalization(self.dense3_b,training=self.training, momentum = self.momentum)\n",
    "                             \n",
    "            self.bn_norm_act3 = tf.nn.elu(self.bn_norm_sup3) # activate the batch norm\n",
    "    \n",
    "    \n",
    "    \n",
    "     # combine with conv_part here\n",
    "\n",
    "            self.drop3_ext = tf.concat([self.dense3, self.conv_outgoing], 1)\n",
    "    \n",
    "    #addition\n",
    "    \n",
    "            self.dense4 = tf.layers.dense(self.drop3_ext, (self.n_neurons - 10), activation=tf.nn.elu,  \n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(self.scale),\n",
    "                           name=\"supplementationDense4\") # add a dense layer\n",
    "    \n",
    "            self.bn_norm_sup4 = tf.layers.batch_normalization(self.dense4,training=self.training, momentum = self.momentum)                  \n",
    "            self.bn_norm_act4 = tf.nn.elu(self.bn_norm_sup4) # activate the batch norm\n",
    "    \n",
    "    \n",
    "            self.drop4 = tf.layers.dropout(self.bn_norm_act4, training = self.training, rate=(1-self.input_keep_prob) )\n",
    "    \n",
    "    # combine with output here\n",
    "    \n",
    "            self.dense4_ext = tf.concat([self.drop4, self.states], 1)\n",
    "    \n",
    "  \n",
    "            self.dense5 = tf.layers.dense(self.dense4_ext, (self.n_neurons - 55), activation=tf.nn.elu,  \n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(self.scale),\n",
    "                           name=\"supplementationDense5\") # add a dense layer\n",
    "    \n",
    "    \n",
    "            self.drop5 = tf.layers.dropout(self.dense5, training = self.training, rate=self.dropout )\n",
    "    \n",
    "            self.dense6 = tf.layers.dense(self.drop5, (self.n_neurons - 55), activation=tf.nn.elu,  \n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(self.scale),\n",
    "                           name=\"supplementationDense6\") # add a dense layer\n",
    "        \n",
    "        with tf.name_scope(\"logits\"):\n",
    "\n",
    "            self.logits_before_bn = tf.layers.dense(self.dense6, self.n_outputs, name=\"outputs_logits\") #logits = tf.layers.dense(states, n_outputs)\n",
    "            self.logits = tf.layers.batch_normalization(self.logits_before_bn,training=self.training, momentum = self.momentum)\n",
    "    \n",
    "    \n",
    "\n",
    "        #  for training of the RNN\n",
    "        with tf.name_scope(\"loss\"):\n",
    "    \n",
    "    \n",
    "    \n",
    "            # sparse softmax, so I don't have to get the ohot\n",
    "            self.xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=self.logits)\n",
    "    \n",
    "            # regularization losses\n",
    "            self.reg_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    \n",
    "            # ignore regularization  it's not helping\n",
    "            self.loss = tf.reduce_mean(self.xentropy)\n",
    "            #\n",
    "            #  activate regularization here\n",
    "            self.loss_aftereg = tf.add_n([self.loss]+ self.reg_loss)\n",
    "    \n",
    "    \n",
    "        with tf.name_scope(\"train\"):\n",
    "    \n",
    "    \n",
    "            self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "            self.learning_rate = tf.train.exponential_decay(self.initial_learning,self.global_step,self.decay_steps,self.decay_rate)\n",
    "            #learning_rate = tf.train.natural_exp_decay(initial_learning,global_step,decay_steps,decay_rate)\n",
    "    \n",
    "            #learning_rate = 0.00074\n",
    "            self.optimizer = tf.contrib.opt.NadamOptimizer(learning_rate=self.learning_rate) # Adamoptimizer implementing the nesterov step \n",
    "            #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            #optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate) # recommended for RNN,  not optimal performance\n",
    "            #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum, use_nesterov=True)\n",
    "            #optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "            #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "            training_op = self.optimizer.minimize(self.loss_aftereg, global_step=self.global_step)\n",
    "\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            self.correct = tf.nn.in_top_k(self.logits, self.y, 1)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct, tf.float32))\n",
    "    \n",
    "        with tf.name_scope(\"logging\"):\n",
    "    \n",
    "            self.accuracy_sum = tf.summary.scalar('accuracy',self.accuracy)\n",
    "            self.loss_sum = tf.summary.scalar('loss',self.loss)\n",
    "    \n",
    "            self.summaries = tf.summary.merge_all()\n",
    "    \n",
    "            # writ to tensorboard\n",
    "            self.train_writer = tf.summary.FileWriter(self.logdir + '/train', tf.get_default_graph())\n",
    "            self.test_writer = tf.summary.FileWriter(self.logdir + '/test')\n",
    "    \n",
    "    \n",
    "        with tf.name_scope(\"predict\"):\n",
    "            self.prediction = tf.argmax(self.logits,1)\n",
    "    \n",
    "        \n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    \n",
    "    def run_autoencoder(self, prefetched_test, X_mess, batch_size = 320, n_epochs = 25, verbose = False, savepath = 'none'):\n",
    "        with tf.Session() as sess:\n",
    "            self.init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                for iteration in range( prefetched_test.shape[0]// (batch_size) ):\n",
    "                    X_batch, y_batch = batch_from_prefetch_samples(prefetched_test,batch_size)\n",
    "                    #X_batch = X_batch.tolist()\n",
    "\n",
    "\n",
    "                    sess.run(a_training_op, feed_dict={self.Aenc_training: True, self.X: X_batch})\n",
    "        \n",
    "        \n",
    "                loss_train = self.a_reconstruction_loss.eval(feed_dict={self.X: X_batch})\n",
    "                loss_val = self.a_reconstruction_loss.eval(feed_dict={self.X: X_mess})\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train, \" val MSE:\", loss_val)    \n",
    "        \n",
    "            if savepath == 'none':\n",
    "                saver.save(sess, \"FirstStep_Autoencoderonly_%s.ckpt\" %epoch)\n",
    "            else:\n",
    "                 saver.save(sess, savepath)\n",
    "\n",
    "    def run_convnet(self, autoencoder_savepath, prefetched_data, X_val, y_val , batch_size = 300 , n_epochs= 420, verbose = True):\n",
    "        print(\"convnet\")\n",
    "        with tf.Session() as sess:\n",
    "            #init.run()\n",
    "            saver.restore(sess,autoencoder_savepath) \n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                for iteration in range( index_df.shape[0] // (batch_size) ):\n",
    "                    X_batch, y_batch = batch_from_prefetch_samples(prefetched_data,batch_size)\n",
    "                    \n",
    "            \n",
    "                    sess.run([self.conv_training_op], feed_dict={self.train_convNet : True, self.X: X_batch, self.y: y_batch, })\n",
    "            \n",
    "            \n",
    "                acc_train = self.accuracy.eval(feed_dict={self.X: X_test, self.y: y_test})\n",
    "                acc_test = self.accuracy.eval(feed_dict={self.X: X_test, self.y: y_test})\n",
    "        \n",
    "                if acc_test >0.6:\n",
    "                    save_path = saver.save(sess, \"Conv_checkpoint/Intermed_conv_auto_%s.ckpt\" %acc_test)\n",
    "            \n",
    "                if verbose:\n",
    "                    print(epoch, \" Conv Train accuracy:\", acc_train, \" test\" , acc_test)\n",
    "         \n",
    "            save_path = saver.save(sess, \"Second_Stage_conv_auto_%s.ckpt\" %(n_epochs) )\n",
    "        \n",
    "    def run(self, convnet_savepath,prefetched_data, X_val, y_val, verbose = False, n_epochs = 420, batch_size = 300):\n",
    "        with tf.Session() as sess:\n",
    "            init.run()\n",
    "            count = 0\n",
    "    \n",
    "    # load the autoencoder to pre-process the data\n",
    "            saver.restore(sess,convnet_savepath) \n",
    "   \n",
    "    \n",
    "            for epoch in range(n_epochs):\n",
    "\n",
    "                for iteration in range( index_df.shape[0] // (batch_size * 8) ):\n",
    "                    X_batch, y_batch = batch_from_prefetch_samples(prefetched_data,batch_size)\n",
    "                    X_batch_aug = augment_batch(X_batch= X_batch, noise_max=0.01, shift_max=8)\n",
    "                    sess.run([self.training_op,self.extra_update_ops], feed_dict={self.training: True, self.X: X_batch_aug, self.y: y_batch})\n",
    "            \n",
    "                    if iteration % 200 == 0:\n",
    "                        step = epoch*iteration\n",
    "                        summary, _ = sess.run([self.summaries,self.extra_update_ops], feed_dict={self.X: X_test, self.y: y_test} )\n",
    "                        self.train_writer.add_summary(summary, step )\n",
    "\n",
    "                        summary, _ = sess.run([self.summaries,self.extra_update_ops], feed_dict={self.X: X_val, self.y: y_val})\n",
    "                        self.test_writer.add_summary(summary,step)\n",
    "                        \n",
    "                        acc_train = self.accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                        acc_val = self.accuracy.eval(feed_dict={X: X_val, y: y_val})\n",
    "        \n",
    "                step = epoch*iteration\n",
    "                summary, _ = sess.run([self.summaries,self.extra_update_ops], feed_dict={self.X: X_test, self.y: y_test} )\n",
    "                self.train_writer.add_summary(summary, step )     \n",
    "                summary, _ = sess.run([self.summaries,self.extra_update_ops], feed_dict={self.X: X_val, self.y: y_val})    \n",
    "                self.test_writer.add_summary(summary,step)\n",
    "        \n",
    "        \n",
    "      \n",
    "                #if count % 5 == 0:\n",
    "                #    save_path = saver.save(sess, \"RNN_4_concat_conv2d_ampdata_1001_savedbycount_%s_%s.ckpt\" %(epoch,acc_mess) )\n",
    "                #count +=1\n",
    "            \n",
    "                acc_val = self.accuracy.eval(feed_dict={self.X: X_val, self.y: y_val}) \n",
    "                \n",
    "                val_pred = self.prediction.eval(feed_dict={self.X: X_val})\n",
    "                multi_acc = multiclass_accuracy(y_val,val_pred)\n",
    "        \n",
    "                if multi_test > 0.98:\n",
    "                    save_path = saver.save(sess, \"RNN__%s.ckpt\" %(multi_acc) )\n",
    "                    print(\"written savepoint %s \" %(multi_acc))\n",
    "        \n",
    "                print(epoch, \"Train accuracy:\", round(acc_train,2), \" - \", round(acc_test,2),\"mediocre :\", \"ma test: \", round(multi_test,2))\n",
    "            \n",
    "            \n",
    "            save_path = saver.save(sess, \"Final_training_auto_conv_RNN.ckpt\")\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict(self,  checkpoint = \"Final_training_auto_conv_RNN.ckpt\"):\n",
    "        print(\"predicting\")\n",
    "        saver.restore(sess, \"Final_training_auto_conv_RNN.ckpt\")\n",
    "\n",
    "        reslist = []\n",
    "    \n",
    "        for group in chunker(data,1000):\n",
    "            results = prediction.eval(feed_dict={X:group})\n",
    "            for element in results.tolist():\n",
    "                reslist.append(element)\n",
    "        self.reslist = reslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Multilayer_RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
